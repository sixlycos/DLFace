"""
Streamlitå‰ç«¯åº”ç”¨ - æä¾›ç”¨æˆ·å‹å¥½çš„ç•Œé¢ï¼Œç”¨äºè§†é¢‘äººç‰©é¢éƒ¨æ›¿æ¢

ä½¿ç”¨Streamlitæ„å»ºçš„Webåº”ç”¨ï¼Œç”¨æˆ·å¯ä¸Šä¼ æºäººè„¸å’Œç›®æ ‡è§†é¢‘ï¼Œé…ç½®æ›¿æ¢å‚æ•°ï¼Œé¢„è§ˆå’Œä¸‹è½½ç»“æœ
"""

import streamlit as st
import os
import cv2
import numpy as np
import tempfile
import time
from pathlib import Path
from typing import Optional, Tuple
import sys
import glob
from PIL import Image
import shutil
import logging

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°è·¯å¾„ä¸­ï¼Œä»¥ä¾¿å¯¼å…¥å…¶ä»–æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æ—¥å¿—é…ç½®å’ŒGPUè¡¥ä¸
from core.logger_config import setup_logger
from core.gpu_patch import apply_gpu_patches

# è®¾ç½®æ—¥å¿—å’Œåº”ç”¨GPUè¡¥ä¸
logger = setup_logger()
gpu_available = apply_gpu_patches()
logger.info(f"GPUå¯ç”¨æ€§: {'å¯ç”¨' if gpu_available else 'ç¦ç”¨'}")

from core.video_processor import VideoProcessor
from core.face_detector import FaceDetector
from core.eye_tracker import EyeTracker
from app.utils import (
    save_uploaded_file, create_thumbnail, plot_side_by_side,
    generate_output_filename, get_video_info, extract_video_preview,
    load_image
)
from core.utils import read_image_with_path_fix


# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
def init_session_state():
    """åˆå§‹åŒ–Streamlitä¼šè¯çŠ¶æ€"""
    # åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„
    os.makedirs("temp", exist_ok=True)
    os.makedirs("temp/faces", exist_ok=True)
    os.makedirs("temp/videos", exist_ok=True)
    os.makedirs("temp/output", exist_ok=True)
    
    if 'processor' not in st.session_state:
        # ä½¿ç”¨å…±äº«çš„GPUæ£€æµ‹ç»“æœ
        use_cuda = gpu_available
        
        if use_cuda:
            # å°è¯•è·å–æ›´è¯¦ç»†çš„GPUä¿¡æ¯
            try:
                import torch
                if torch.cuda.is_available():
                    gpu_info = f"GPU: {torch.cuda.get_device_name(0)}"
                    st.sidebar.success(f"âœ… å·²æ£€æµ‹åˆ°GPUå¹¶å¯ç”¨åŠ é€Ÿ\n{gpu_info}")
                    
                    # è¾“å‡ºè¯¦ç»†GPUä¿¡æ¯
                    memory_allocated = torch.cuda.memory_allocated(0) / 1024**2
                    memory_reserved = torch.cuda.memory_reserved(0) / 1024**2
                    st.sidebar.info(f"GPUå†…å­˜: å·²åˆ†é… {memory_allocated:.1f}MB / å·²é¢„ç•™ {memory_reserved:.1f}MB")
                else:
                    try:
                        import tensorflow as tf
                        gpus = tf.config.list_physical_devices('GPU')
                        if gpus:
                            gpu_info = f"GPUæ•°é‡: {len(gpus)}"
                            st.sidebar.success(f"âœ… å·²æ£€æµ‹åˆ°GPUå¹¶å¯ç”¨åŠ é€Ÿ\n{gpu_info}")
                        else:
                            st.sidebar.warning("âš ï¸ TensorFlowæœªæ£€æµ‹åˆ°GPUï¼Œä½†ç¯å¢ƒå˜é‡æŒ‡ç¤ºGPUå¯ç”¨")
                    except:
                        st.sidebar.warning("âš ï¸ æ— æ³•è·å–è¯¦ç»†GPUä¿¡æ¯ï¼Œä½†å·²å¯ç”¨GPUåŠ é€Ÿ")
            except:
                st.sidebar.warning("âš ï¸ æ— æ³•è·å–è¯¦ç»†GPUä¿¡æ¯ï¼Œä½†å·²å¯ç”¨GPUåŠ é€Ÿ")
        else:
            st.sidebar.warning("âš ï¸ æœªæ£€æµ‹åˆ°å…¼å®¹çš„GPUï¼Œä½¿ç”¨CPUæ¨¡å¼")
        
        # åˆå§‹åŒ–è§†é¢‘å¤„ç†å™¨
        st.session_state.processor = VideoProcessor(use_cuda=use_cuda)
        st.session_state.face_detector = FaceDetector(use_gpu=use_cuda)
        st.session_state.eye_tracker = EyeTracker(use_cuda=use_cuda)
    
    # å…¶ä»–ä¼šè¯çŠ¶æ€å˜é‡
    if 'source_face_path' not in st.session_state:
        st.session_state.source_face_path = None
    
    if 'target_video_path' not in st.session_state:
        st.session_state.target_video_path = None
    
    if 'output_video_path' not in st.session_state:
        st.session_state.output_video_path = None
    
    if 'processing_status' not in st.session_state:
        st.session_state.processing_status = None
    
    if 'model_loaded' not in st.session_state:
        st.session_state.model_loaded = False
    
    if 'preview_image' not in st.session_state:
        st.session_state.preview_image = None


def load_model():
    """åŠ è½½æ¨¡å‹"""
    with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹..."):
        # æ¨¡å‹è·¯å¾„
        model_path = os.path.join("data", "models", "face_swap_model.h5")
        
        # å¦‚æœæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ˜¾ç¤ºè­¦å‘Š
        if not os.path.exists(model_path):
            st.warning(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            st.info("ç³»ç»Ÿå°†ä½¿ç”¨æœªè®­ç»ƒçš„æ¨¡å‹ï¼Œæ•ˆæœå¯èƒ½ä¸ä½³ã€‚è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        # åŠ è½½æ¨¡å‹
        st.session_state.processor.load_model(model_path)
        st.session_state.model_loaded = True
        st.success("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")


def process_source_face(source_face_path):
    """å¤„ç†æºäººè„¸"""
    if source_face_path and os.path.exists(source_face_path):
        # è¯»å–æºäººè„¸å›¾åƒ
        source_face = load_image(source_face_path)
        
        if source_face is not None:
            # åœ¨UIä¸­æ˜¾ç¤ºæºäººè„¸
            st.image(cv2.cvtColor(source_face, cv2.COLOR_BGR2RGB), 
                    caption="æºäººè„¸", use_column_width=True)
            return True
        else:
            st.error("æ— æ³•è¯»å–æºäººè„¸å›¾åƒ")
    else:
        st.error("æºäººè„¸å›¾åƒè·¯å¾„æ— æ•ˆ")
    
    return False


def upload_source_face():
    """ä¸Šä¼ æºäººè„¸"""
    st.write("è¯·ä¸Šä¼ ä¸€å¼ åŒ…å«æ¸…æ™°äººè„¸çš„ç…§ç‰‡ï¼Œè¯¥äººè„¸å°†è¢«ç”¨äºæ›¿æ¢è§†é¢‘ä¸­çš„äººè„¸ã€‚")
    st.write("å»ºè®®ä½¿ç”¨æ­£é¢ã€å…‰çº¿è‰¯å¥½ã€è¡¨æƒ…è‡ªç„¶çš„ç…§ç‰‡ï¼Œä»¥è·å¾—æœ€ä½³æ•ˆæœã€‚")
    
    uploaded_file = st.file_uploader("é€‰æ‹©å›¾ç‰‡æ–‡ä»¶", type=["jpg", "jpeg", "png"], key="source_face_uploader")
    
    if uploaded_file is not None:
        # æ˜¾ç¤ºä¸Šä¼ ä¿¡æ¯
        st.success(f"å·²æ¥æ”¶æ–‡ä»¶: {uploaded_file.name}")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs("temp/faces", exist_ok=True)
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        source_face_path = save_uploaded_file(uploaded_file, save_dir="temp/faces")
        
        if not source_face_path:
            st.error("å›¾ç‰‡ä¿å­˜å¤±è´¥ï¼Œè¯·é‡è¯•")
            return False
        
        # å°†è·¯å¾„ä¿å­˜åˆ°session state
        st.session_state.source_face_path = source_face_path
        
        # è¯»å–æºäººè„¸å›¾åƒ
        source_face = load_image(source_face_path)
        
        if source_face is not None:
            # æ£€æµ‹äººè„¸
            with st.spinner("æ­£åœ¨åˆ†æäººè„¸..."):
                faces = st.session_state.face_detector.detect_faces(source_face)
            
            # åœ¨UIä¸­æ˜¾ç¤ºæºäººè„¸
            st.image(cv2.cvtColor(source_face, cv2.COLOR_BGR2RGB), 
                    caption="ä¸Šä¼ çš„æºäººè„¸", use_column_width=True)
            
            # æ˜¾ç¤ºäººè„¸æ£€æµ‹ç»“æœ
            if faces:
                st.success(f"æ£€æµ‹åˆ° {len(faces)} ä¸ªäººè„¸")
                
                # å¦‚æœæ£€æµ‹åˆ°å¤šä¸ªäººè„¸ï¼Œæ˜¾ç¤ºè­¦å‘Š
                if len(faces) > 1:
                    st.warning("å›¾ç‰‡ä¸­åŒ…å«å¤šä¸ªäººè„¸ã€‚ä¸ºè·å¾—æœ€ä½³æ•ˆæœï¼Œå»ºè®®ä½¿ç”¨åªåŒ…å«ä¸€ä¸ªæ¸…æ™°äººè„¸çš„ç…§ç‰‡ã€‚")
            else:
                st.warning("æœªæ£€æµ‹åˆ°äººè„¸ï¼Œè¯·ä¸Šä¼ åŒ…å«æ¸…æ™°äººè„¸çš„ç…§ç‰‡")
            
            # è®¾ç½®æˆåŠŸä¸Šä¼ æ ‡å¿—
            st.session_state.source_face_uploaded = True
            
            # åˆå§‹åŒ–å¤„ç†å™¨
            if "processor" not in st.session_state:
                with st.spinner("åˆå§‹åŒ–å¤„ç†å™¨..."):
                    processor = VideoProcessor()
                    st.session_state.processor = processor
                    
            # åŠ è½½æºäººè„¸
            st.session_state.processor.load_source_face(source_face_path)
            
            return True
        else:
            st.error("æ— æ³•è¯»å–æºäººè„¸å›¾åƒ")
            st.info("è¯·ç¡®ä¿ä¸Šä¼ çš„æ˜¯æœ‰æ•ˆçš„å›¾ç‰‡æ–‡ä»¶ã€‚æ”¯æŒçš„æ ¼å¼åŒ…æ‹¬JPGã€JPEGå’ŒPNGã€‚")
    
    return False


def upload_target_video():
    """ä¸Šä¼ ç›®æ ‡è§†é¢‘"""
    st.write("è¯·ä¸Šä¼ ä¸€ä¸ªåŒ…å«äººè„¸çš„è§†é¢‘æ–‡ä»¶ï¼Œè¯¥è§†é¢‘ä¸­çš„äººè„¸å°†è¢«æ›¿æ¢ä¸ºæºäººè„¸ã€‚")
    st.write("æ”¯æŒçš„æ ¼å¼: MP4, AVI, MOV")
    
    target_video_file = st.file_uploader("é€‰æ‹©è§†é¢‘æ–‡ä»¶", type=["mp4", "avi", "mov"], key="target_video_uploader")
    
    if target_video_file is not None:
        # æ˜¾ç¤ºä¸Šä¼ ä¿¡æ¯
        st.success(f"å·²æ¥æ”¶æ–‡ä»¶: {target_video_file.name}")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs("temp/videos", exist_ok=True)
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        target_video_path = save_uploaded_file(target_video_file, save_dir="temp/videos")
        if not target_video_path:
            st.error("è§†é¢‘ä¿å­˜å¤±è´¥ï¼Œè¯·é‡è¯•")
            return False
            
        st.session_state.target_video_path = target_video_path
        
        # è·å–è§†é¢‘ä¿¡æ¯
        try:
            video_info = get_video_info(target_video_path)
            
            # æ˜¾ç¤ºè§†é¢‘ä¿¡æ¯
            col1, col2, col3 = st.columns(3)
            col1.metric("è§†é¢‘æ—¶é•¿", f"{video_info['duration']}ç§’")
            col2.metric("å¸§æ•°", f"{video_info['frame_count']}")
            col3.metric("åˆ†è¾¨ç‡", f"{video_info['width']}x{video_info['height']}")
            
            # æå–é¢„è§ˆå¸§
            with st.spinner("æ­£åœ¨å¤„ç†è§†é¢‘é¢„è§ˆ..."):
                preview_frames = extract_video_preview(target_video_path, frame_count=3)
            
            if preview_frames:
                # æ˜¾ç¤ºé¢„è§ˆå¸§
                st.write("è§†é¢‘é¢„è§ˆ:")
                cols = st.columns(len(preview_frames))
                for i, frame in enumerate(preview_frames):
                    # åˆ›å»ºç¼©ç•¥å›¾
                    thumbnail = create_thumbnail(frame, max_size=200)
                    
                    # åœ¨å¯¹åº”çš„åˆ—ä¸­æ˜¾ç¤ºç¼©ç•¥å›¾
                    cols[i].image(cv2.cvtColor(thumbnail, cv2.COLOR_BGR2RGB))
                    
                    # å°è¯•æ£€æµ‹äººè„¸
                    faces = st.session_state.face_detector.detect_faces(frame)
                    if faces:
                        cols[i].success(f"æ£€æµ‹åˆ° {len(faces)} ä¸ªäººè„¸")
                    else:
                        cols[i].warning("æœªæ£€æµ‹åˆ°äººè„¸")
            
            return True
        except Exception as e:
            st.error(f"âš ï¸ è¯»å–è§†é¢‘æ—¶å‡ºé”™: {str(e)}")
            st.info("è¯·ç¡®ä¿ä¸Šä¼ çš„è§†é¢‘æ–‡ä»¶å®Œæ•´ä¸”æ ¼å¼æ­£ç¡®ã€‚å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·å°è¯•è½¬æ¢è§†é¢‘æ ¼å¼æˆ–ä½¿ç”¨å¦ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ã€‚")
            return False
    
    return False


def process_single_frame():
    """å¤„ç†å•å¸§é¢„è§ˆ"""
    if not st.session_state.model_loaded:
        st.warning("âš ï¸ è¯·å…ˆåŠ è½½æ¨¡å‹")
        return
    
    if st.session_state.source_face_path is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æºäººè„¸å›¾åƒ")
        return
    
    if st.session_state.target_video_path is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ ç›®æ ‡è§†é¢‘")
        return
    
    # é¢„è§ˆè®¾ç½®
    preview_col1, preview_col2 = st.columns(2)
    
    with preview_col1:
        # è‡ªåŠ¨å¯»æ‰¾äººè„¸å¸§é€‰é¡¹
        auto_find_face = st.checkbox("è‡ªåŠ¨å®šä½åˆ°æœ‰äººè„¸çš„å¸§", value=True)
        max_search_frames = st.slider("æœ€å¤§æœç´¢å¸§æ•°", min_value=10, max_value=500, value=100, step=10,
                                    help="è‡ªåŠ¨æœç´¢äººè„¸æ—¶æœ€å¤šæ£€æŸ¥çš„å¸§æ•°")
    
    with preview_col2:
        # æ‰‹åŠ¨é€‰æ‹©å¸§
        manual_frame = st.number_input("æ‰‹åŠ¨é€‰æ‹©å¸§", value=0, min_value=0, step=1, 
                                     help="è¾“å…¥è¦æŸ¥çœ‹çš„ç‰¹å®šå¸§ç¼–å·")
    
    # æå–è§†é¢‘çš„å¸§
    cap = cv2.VideoCapture(st.session_state.target_video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_rate = cap.get(cv2.CAP_PROP_FPS)
    
    # æ˜¾ç¤ºè§†é¢‘ä¿¡æ¯
    st.info(f"è§†é¢‘æ€»å¸§æ•°: {total_frames}, å¸§ç‡: {frame_rate:.2f}fps, æ—¶é•¿: {total_frames/frame_rate:.2f}ç§’")
    
    # è·å–è¦å¤„ç†çš„å¸§
    selected_frame = manual_frame
    frame = None
    
    # è‡ªåŠ¨å¯»æ‰¾åŒ…å«äººè„¸çš„å¸§
    if auto_find_face:
        with st.spinner("æ­£åœ¨æœç´¢åŒ…å«äººè„¸çš„å¸§..."):
            frame_with_face = None
            frame_idx = 0
            
            # é™åˆ¶æœç´¢èŒƒå›´
            search_limit = min(total_frames, max_search_frames)
            
            # åˆ›å»ºä¸´æ—¶è¿›åº¦æ¡
            progress = st.progress(0)
            
            for i in range(search_limit):
                # æ›´æ–°è¿›åº¦
                progress.progress(i / search_limit)
                
                # è®¾ç½®å¸§ä½ç½®
                cap.set(cv2.CAP_PROP_POS_FRAMES, i)
                ret, current_frame = cap.read()
                
                if not ret:
                    break
                
                # æ£€æµ‹äººè„¸
                faces = st.session_state.face_detector.detect_faces(current_frame)
                
                if faces:
                    frame_with_face = current_frame
                    frame_idx = i
                    selected_frame = i
                    break
            
            # æ¸…é™¤è¿›åº¦æ¡
            progress.empty()
            
            if frame_with_face is not None:
                st.success(f"âœ… åœ¨ç¬¬ {frame_idx} å¸§æ‰¾åˆ°äººè„¸")
                frame = frame_with_face
            else:
                st.warning("âš ï¸ åœ¨æœç´¢èŒƒå›´å†…æœªæ‰¾åˆ°åŒ…å«äººè„¸çš„å¸§")
                # é‡ç½®å¸§ä½ç½®å¹¶è¯»å–ç¬¬ä¸€å¸§
                cap.set(cv2.CAP_PROP_POS_FRAMES, selected_frame)
                ret, frame = cap.read()
    else:
        # æ‰‹åŠ¨é€‰æ‹©å¸§
        cap.set(cv2.CAP_PROP_POS_FRAMES, selected_frame)
        ret, frame = cap.read()
    
    cap.release()
    
    if not ret or frame is None:
        st.error("âš ï¸ æ— æ³•è¯»å–è§†é¢‘å¸§")
        return
    
    # åŒºåŸŸé™å®šåŠŸèƒ½
    use_roi = st.checkbox("é™å®šäººè„¸æ£€æµ‹åŒºåŸŸï¼ˆé¢„è§ˆï¼‰", value=False)
    roi = None
    
    if use_roi:
        # åˆ›å»ºä¸¤åˆ—å¸ƒå±€ç”¨äºè¾“å…¥ROI
        height, width = frame.shape[:2]
        roi_col1, roi_col2 = st.columns(2)
        
        with roi_col1:
            roi_x = st.number_input("Xåæ ‡", value=width//4, min_value=0, max_value=width-1)
            roi_w = st.number_input("å®½åº¦", value=width//2, min_value=1, max_value=width)
        
        with roi_col2:
            roi_y = st.number_input("Yåæ ‡", value=height//4, min_value=0, max_value=height-1)
            roi_h = st.number_input("é«˜åº¦", value=height//2, min_value=1, max_value=height)
        
        roi = (roi_x, roi_y, roi_w, roi_h)
        
        # ç»˜åˆ¶ROIåŒºåŸŸåˆ°å¸§ä¸Š
        preview_frame = frame.copy()
        cv2.rectangle(preview_frame, (roi_x, roi_y), (roi_x + roi_w, roi_y + roi_h), (0, 255, 0), 2)
        frame = preview_frame
    
    # å¤„ç†é€‰é¡¹
    st.subheader("å¤„ç†é€‰é¡¹")
    
    col1, col2 = st.columns(2)
    
    with col1:
        smooth_factor = st.slider("è¾¹ç¼˜å¹³æ»‘ç¨‹åº¦(é¢„è§ˆ)", min_value=0.0, max_value=1.0, value=0.5, step=0.1)
        use_eye_optimization = st.checkbox("å¯ç”¨çœ¼éƒ¨ä¼˜åŒ–(é¢„è§ˆ)", value=True)
    
    with col2:
        color_correction = st.checkbox("é¢œè‰²æ ¡æ­£(é¢„è§ˆ)", value=True)
        face_enhancement = st.checkbox("é¢éƒ¨å¢å¼º(é¢„è§ˆ)", value=True)
    
    # æ›´æ–°åå¤„ç†å™¨å‚æ•°
    st.session_state.processor.post_processor.smooth_factor = smooth_factor
    st.session_state.processor.post_processor.color_correction = color_correction
    st.session_state.processor.post_processor.enhance_face = face_enhancement
    
    # å¤„ç†å¸§
    with st.spinner("å¤„ç†ä¸­..."):
        # æ£€æµ‹äººè„¸
        faces = st.session_state.face_detector.detect_faces(frame, roi)
        
        if not faces:
            st.error("âš ï¸ æœªåœ¨è§†é¢‘å¸§ä¸­æ£€æµ‹åˆ°äººè„¸")
            # æ˜¾ç¤ºåŸå§‹å¸§
            st.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), caption="åŸå§‹å¸§")
            return
        
        # å¤„ç†äººè„¸
        result_frame = frame.copy()
        
        for face in faces:
            # æå–ç›®æ ‡äººè„¸
            target_face = st.session_state.face_detector.align_face(frame, face, handle_occlusion=True)
            
            # è·å–ç›®æ ‡äººè„¸ç‰¹å¾ç‚¹
            target_landmarks = face.get('landmarks', [])
            
            # å¦‚æœæœ‰çœ¼éƒ¨è·Ÿè¸ªå™¨ï¼Œä¼˜åŒ–çœ¼ç¥æ–¹å‘
            if use_eye_optimization and st.session_state.eye_tracker is not None and len(target_landmarks) > 0:
                try:
                    # æ£€æµ‹çœ¼ç›ä½ç½®
                    source_face = cv2.imread(st.session_state.source_face_path)
                    source_eyes = st.session_state.eye_tracker.detect_eyes(source_face)
                    target_eyes = st.session_state.eye_tracker.detect_eyes(target_face)
                    
                    # ä¼˜åŒ–çœ¼ç¥æ–¹å‘
                    if source_eyes and target_eyes:
                        optimized_source_face = st.session_state.eye_tracker.optimize_eye_direction(
                            source_eyes, target_eyes, source_face, target_face
                        )
                        
                        # å¢å¼ºçœ¼ç›
                        enhanced_source_face = st.session_state.eye_tracker.enhance_eyes(
                            optimized_source_face, source_eyes
                        )
                    else:
                        enhanced_source_face = source_face
                except Exception as e:
                    st.warning(f"çœ¼éƒ¨è·Ÿè¸ªä¼˜åŒ–å¤±è´¥: {str(e)}")
                    enhanced_source_face = cv2.imread(st.session_state.source_face_path)
            else:
                # ç›´æ¥åŠ è½½æºäººè„¸å›¾åƒ
                enhanced_source_face = cv2.imread(st.session_state.source_face_path)
            
            # æ‰§è¡Œé¢éƒ¨æ›¿æ¢
            swapped_face = st.session_state.processor.face_swapper.swap_face(
                enhanced_source_face, target_face
            )
            
            if swapped_face is None:
                st.warning("âš ï¸ é¢éƒ¨æ›¿æ¢å¤±è´¥")
                continue
            
            # åå¤„ç†
            face_mask = None
            if hasattr(st.session_state.processor.face_swapper, '_create_face_mask'):
                face_mask = st.session_state.processor.face_swapper._create_face_mask(target_face)
            
            processed_face = st.session_state.processor.post_processor.process(
                target_face, swapped_face, face_mask, target_landmarks
            )
            
            # å°†æ›¿æ¢åçš„äººè„¸æ”¾å›åŸå›¾
            bbox = face.get('bbox')
            if bbox:
                x, y, w, h = bbox
                # è°ƒæ•´å¤§å°ä»¥åŒ¹é…åŸå§‹äººè„¸åŒºåŸŸ
                processed_face = cv2.resize(processed_face, (w, h))
                # åˆ›å»ºæ©ç 
                mask = np.zeros((h, w), dtype=np.uint8)
                center = (w // 2, h // 2)
                axes = (w // 2 - w // 8, h // 2 - h // 8)
                cv2.ellipse(mask, center, axes, 0, 0, 360, 255, -1)
                mask = cv2.GaussianBlur(mask, (31, 31), 0)
                # æ··åˆ
                mask_3d = np.stack([mask, mask, mask], axis=2) / 255.0
                
                # ç¡®ä¿roiå’Œprocessed_faceå°ºå¯¸ä¸€è‡´
                roi = result_frame[y:y+h, x:x+w].copy()
                if roi.shape != processed_face.shape:
                    processed_face = cv2.resize(processed_face, (roi.shape[1], roi.shape[0]))
                    # é‡æ–°åˆ›å»ºæ©ç ä»¥åŒ¹é…è°ƒæ•´åçš„å¤§å°
                    mask = cv2.resize(mask, (roi.shape[1], roi.shape[0]))
                    mask_3d = np.stack([mask, mask, mask], axis=2) / 255.0
                
                # æ··åˆå›¾åƒ
                roi = (roi * (1 - mask_3d) + processed_face * mask_3d).astype(np.uint8)
                result_frame[y:y+h, x:x+w] = roi
        
        # æ˜¾ç¤ºå¤„ç†ç»“æœ
        st.subheader("å¤„ç†ç»“æœå¯¹æ¯”")
        col1, col2 = st.columns(2)
        
        with col1:
            st.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), caption="åŸå§‹å¸§")
        
        with col2:
            st.image(cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB), caption="å¤„ç†åå¸§")
        
        # ä¿å­˜å¤„ç†åçš„å¸§åˆ°ä¼šè¯çŠ¶æ€ä¸­ï¼Œä»¥ä¾¿åç»­ä½¿ç”¨
        st.session_state.preview_image = result_frame


def process_video():
    """å¤„ç†è§†é¢‘"""
    if not st.session_state.model_loaded:
        st.warning("âš ï¸ è¯·å…ˆåŠ è½½æ¨¡å‹")
        return
    
    if st.session_state.source_face_path is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æºäººè„¸å›¾åƒ")
        return
    
    if st.session_state.target_video_path is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ ç›®æ ‡è§†é¢‘")
        return
    
    # å¤„ç†å‚æ•°
    st.subheader("è§†é¢‘å¤„ç†å‚æ•°")
    
    # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
    col1, col2 = st.columns(2)
    
    with col1:
        # èµ·å§‹å¸§å’Œç»“æŸå¸§
        start_frame = st.number_input("èµ·å§‹å¸§", value=0, min_value=0, step=1)
        fps = st.number_input("è¾“å‡ºå¸§ç‡", value=0, min_value=0, help="0è¡¨ç¤ºä½¿ç”¨åŸå§‹å¸§ç‡")
    
    with col2:
        # é™åˆ¶å¸§æ•°
        frame_limit = st.number_input("å¤„ç†å¸§æ•°é™åˆ¶", value=0, min_value=0, help="0è¡¨ç¤ºä¸é™åˆ¶")
        resolution = st.selectbox("è¾“å‡ºåˆ†è¾¨ç‡", ["åŸå§‹åˆ†è¾¨ç‡", "1080p", "720p", "480p"])
    
    # åŒºåŸŸé™å®šåŠŸèƒ½
    st.subheader("åŒºåŸŸé™å®šï¼ˆå¯é€‰ï¼‰")
    use_roi = st.checkbox("é™å®šäººè„¸æ£€æµ‹åŒºåŸŸ", value=False)
    roi = None
    
    if use_roi:
        # æ˜¾ç¤ºç›®æ ‡è§†é¢‘çš„ç¬¬ä¸€å¸§ï¼Œè®©ç”¨æˆ·é€‰æ‹©åŒºåŸŸ
        cap = cv2.VideoCapture(st.session_state.target_video_path)
        ret, frame = cap.read()
        cap.release()
        
        if ret:
            # è°ƒæ•´å›¾åƒå¤§å°ä»¥é€‚åº”ç•Œé¢
            max_width = 600
            height, width = frame.shape[:2]
            scale = max_width / width if width > max_width else 1.0
            if scale < 1.0:
                frame = cv2.resize(frame, (int(width * scale), int(height * scale)))
            
            # æ˜¾ç¤ºç¬¬ä¸€å¸§
            st.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), caption="ç›®æ ‡è§†é¢‘ç¬¬ä¸€å¸§")
            
            # åˆ›å»ºä¸¤åˆ—å¸ƒå±€ç”¨äºè¾“å…¥ROI
            roi_col1, roi_col2 = st.columns(2)
            
            with roi_col1:
                roi_x = st.number_input("Xåæ ‡", value=width//4, min_value=0, max_value=width-1)
                roi_w = st.number_input("å®½åº¦", value=width//2, min_value=1, max_value=width)
            
            with roi_col2:
                roi_y = st.number_input("Yåæ ‡", value=height//4, min_value=0, max_value=height-1)
                roi_h = st.number_input("é«˜åº¦", value=height//2, min_value=1, max_value=height)
            
            # è°ƒæ•´åæ ‡åˆ°åŸå§‹å›¾åƒå¤§å°
            if scale < 1.0:
                roi_x = int(roi_x / scale)
                roi_y = int(roi_y / scale)
                roi_w = int(roi_w / scale)
                roi_h = int(roi_h / scale)
            
            roi = (roi_x, roi_y, roi_w, roi_h)
            
            # ç»˜åˆ¶ROIåŒºåŸŸ
            preview_frame = frame.copy()
            cv2.rectangle(preview_frame, (int(roi_x*scale), int(roi_y*scale)), 
                         (int((roi_x+roi_w)*scale), int((roi_y+roi_h)*scale)), (0, 255, 0), 2)
            st.image(cv2.cvtColor(preview_frame, cv2.COLOR_BGR2RGB), caption="é€‰å®šåŒºåŸŸé¢„è§ˆ")
    
    # çœ¼éƒ¨ä¼˜åŒ–é€‰é¡¹
    st.subheader("çœ¼éƒ¨ä¼˜åŒ–")
    use_eye_optimization = st.checkbox("å¯ç”¨çœ¼éƒ¨ä¼˜åŒ–", value=True)
    
    if use_eye_optimization:
        eye_smooth_factor = st.slider("çœ¼éƒ¨å¹³æ»‘ç¨‹åº¦", min_value=0.0, max_value=1.0, value=0.5, step=0.1)
        pupil_enhancement = st.slider("ç³å­”å¢å¼ºç¨‹åº¦", min_value=0.0, max_value=1.0, value=0.7, step=0.1)
    
    # è´¨é‡è¯„ä¼°é€‰é¡¹
    st.subheader("è´¨é‡è¯„ä¼°")
    enable_quality_assessment = st.checkbox("å¯ç”¨è´¨é‡è¯„ä¼°", value=False)
    
    # åå¤„ç†é€‰é¡¹
    st.subheader("åå¤„ç†é€‰é¡¹")
    smooth_factor = st.slider("è¾¹ç¼˜å¹³æ»‘ç¨‹åº¦", min_value=0.0, max_value=1.0, value=0.5, step=0.1)
    color_correction = st.checkbox("é¢œè‰²æ ¡æ­£", value=True)
    face_enhancement = st.checkbox("é¢éƒ¨å¢å¼º", value=True)
    
    # ç¡®å®šè¾“å‡ºåˆ†è¾¨ç‡
    output_resolution = None
    if resolution == "1080p":
        output_resolution = (1920, 1080)
    elif resolution == "720p":
        output_resolution = (1280, 720)
    elif resolution == "480p":
        output_resolution = (854, 480)
    
    # è¿›åº¦çŠ¶æ€å®¹å™¨
    progress_placeholder = st.empty()
    
    # å¤„ç†æŒ‰é’®
    if st.button("å¼€å§‹å¤„ç†è§†é¢‘"):
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("temp/output", exist_ok=True)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        output_filename = generate_output_filename(
            st.session_state.source_face_path, 
            st.session_state.target_video_path
        )
        output_path = os.path.join("temp/output", output_filename)
        
        # æ›´æ–°åå¤„ç†å™¨å‚æ•°
        st.session_state.processor.post_processor.smooth_factor = smooth_factor
        st.session_state.processor.post_processor.color_correction = color_correction
        st.session_state.processor.post_processor.enhance_face = face_enhancement
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = progress_placeholder.progress(0)
        status_text = st.empty()
        status_text.text("å‡†å¤‡å¤„ç†è§†é¢‘...")
        
        # åˆ›å»ºä¸€ä¸ªå®šæ—¶æ›´æ–°è¿›åº¦çš„è¾…åŠ©å‡½æ•°
        def update_progress():
            if hasattr(st.session_state.processor, 'progress'):
                progress = st.session_state.processor.progress
                progress_bar.progress(progress)
                status_text.text(f"å¤„ç†ä¸­... {progress:.1%}")
                
                # å¦‚æœå¤„ç†å®Œæˆ
                if progress >= 1.0:
                    status_text.text("å¤„ç†å®Œæˆï¼")
                    return True
            return False
        
        # å¤„ç†è§†é¢‘
        try:
            # ç¡®å®šç»“æŸå¸§
            end_frame = None if frame_limit == 0 else start_frame + frame_limit
            output_fps = None if fps == 0 else fps
            
            # æ·»åŠ è¿›åº¦æ˜¾ç¤ºå˜é‡
            st.session_state.processing_status = "å¤„ç†ä¸­"
            st.session_state.processing_error = None
            st.session_state.output_video_path = None
            
            # å¯åŠ¨å¤„ç†çº¿ç¨‹
            def process_thread():
                try:
                    output_path_result = st.session_state.processor.process_video(
                        st.session_state.source_face_path,
                        st.session_state.target_video_path,
                        output_path,
                        start_frame=start_frame,
                        end_frame=end_frame,
                        fps=output_fps,
                        resolution=output_resolution,
                        show_progress=True,
                        roi=roi,
                        enable_quality_assessment=enable_quality_assessment
                    )
                    # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ–¹å¼æ›´æ–°ç»“æœ
                    with st.session_state._lock if hasattr(st.session_state, '_lock') else threading.Lock():
                        st.session_state.output_video_path = output_path_result
                        st.session_state.processing_status = "å®Œæˆ"
                except Exception as e:
                    # æ•è·å¹¶è®°å½•é”™è¯¯
                    error_msg = str(e)
                    with st.session_state._lock if hasattr(st.session_state, '_lock') else threading.Lock():
                        st.session_state.processing_error = error_msg
                        st.session_state.processing_status = "é”™è¯¯"
            
            # å¯åŠ¨çº¿ç¨‹
            import threading
            if not hasattr(st.session_state, '_lock'):
                st.session_state._lock = threading.Lock()
                
            processing_thread = threading.Thread(target=process_thread)
            processing_thread.daemon = True
            processing_thread.start()
            
            # å®æ—¶è¿›åº¦æ˜¾ç¤ºæ·»åŠ å‰©ä½™æ—¶é—´ä¼°è®¡
            start_time = time.time()
            last_progress = 0
            speeds = []  # å­˜å‚¨è¿‘æœŸå¤„ç†é€Ÿåº¦
            
            while st.session_state.processing_status == "å¤„ç†ä¸­":
                current_progress = st.session_state.processor.progress
                
                # è®¡ç®—å¤„ç†é€Ÿåº¦å’Œé¢„è®¡å‰©ä½™æ—¶é—´
                if current_progress > last_progress:
                    elapsed = time.time() - start_time
                    progress_diff = current_progress - last_progress
                    if progress_diff > 0 and elapsed > 0:
                        speed = progress_diff / elapsed
                        speeds.append(speed)
                        # åªä¿ç•™æœ€è¿‘10ä¸ªé€Ÿåº¦æ ·æœ¬
                        if len(speeds) > 10:
                            speeds.pop(0)
                        
                        # è®¡ç®—å¹³å‡é€Ÿåº¦å’Œé¢„è®¡å‰©ä½™æ—¶é—´
                        avg_speed = sum(speeds) / len(speeds)
                        remaining_progress = 1.0 - current_progress
                        estimated_time = remaining_progress / avg_speed if avg_speed > 0 else 0
                        
                        # æ›´æ–°è¿›åº¦æ˜¾ç¤º
                        progress_bar.progress(current_progress)
                        status_text.text(f"å¤„ç†ä¸­... {current_progress:.1%} å·²å®Œæˆ | é¢„è®¡å‰©ä½™æ—¶é—´: {estimated_time:.1f}ç§’ | å¸§ç‡: {int(avg_speed * 100 * st.session_state.processor.frames_to_process)}å¸§/ç§’")
                        
                        # é‡ç½®
                        last_progress = current_progress
                        start_time = time.time()
                    
                # é˜²æ­¢UIå¡æ­»
                time.sleep(0.5)
                
                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if st.session_state.processing_status in ["å®Œæˆ", "é”™è¯¯"]:
                    break
            
            # å¤„ç†å®Œæˆæˆ–å‡ºé”™
            if st.session_state.processing_status == "é”™è¯¯":
                st.error(f"å¤„ç†è§†é¢‘æ—¶å‡ºé”™: {st.session_state.processing_error}")
                
            elif st.session_state.processing_status == "å®Œæˆ":
                progress_bar.progress(1.0)
                status_text.text("å¤„ç†å®Œæˆï¼")
                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æ–‡ä»¶å†™å…¥å®Œæˆ
                time.sleep(1)
                st.experimental_rerun()
                
        except Exception as e:
            st.error(f"å¯åŠ¨å¤„ç†æ—¶å‡ºé”™: {str(e)}")
    
    # å¦‚æœå¤„ç†å®Œæˆï¼Œæ˜¾ç¤ºä¸‹è½½é“¾æ¥å’Œç»“æœ
    if st.session_state.processing_status == "å®Œæˆ" and st.session_state.output_video_path and os.path.exists(st.session_state.output_video_path):
        st.success("âœ… è§†é¢‘å¤„ç†å®Œæˆï¼")
        
        # æ˜¾ç¤ºè§†é¢‘é¢„è§ˆ
        st.video(st.session_state.output_video_path)
        
        # ä¸‹è½½é“¾æ¥
        with open(st.session_state.output_video_path, "rb") as file:
            st.download_button(
                label="ä¸‹è½½å¤„ç†åçš„è§†é¢‘",
                data=file,
                file_name=os.path.basename(st.session_state.output_video_path),
                mime="video/mp4"
            )
        
        # æ˜¾ç¤ºè´¨é‡è¯„ä¼°ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        if enable_quality_assessment:
            quality_dir = os.path.join(os.path.dirname(st.session_state.output_video_path), "quality_assessment")
            if os.path.exists(quality_dir):
                st.subheader("è´¨é‡è¯„ä¼°ç»“æœ")
                
                # æ˜¾ç¤ºè´¨é‡è¯„ä¼°æŠ¥å‘Š
                report_path = os.path.join(quality_dir, "quality_report.txt")
                if os.path.exists(report_path):
                    with open(report_path, "r") as file:
                        report_content = file.read()
                        st.text_area("è¯„ä¼°æŠ¥å‘Š", report_content, height=200)
                
                # æ˜¾ç¤ºè´¨é‡æŒ‡æ ‡å›¾è¡¨
                metrics_path = os.path.join(quality_dir, "quality_metrics.png")
                if os.path.exists(metrics_path):
                    st.image(metrics_path, caption="è´¨é‡æŒ‡æ ‡å›¾è¡¨")
                
                # æ˜¾ç¤ºå¯è§†åŒ–ç»“æœ
                st.subheader("è´¨é‡å¯è§†åŒ–")
                vis_files = sorted(glob.glob(os.path.join(quality_dir, "quality_vis_*.png")))
                if vis_files:
                    for vis_file in vis_files[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                        st.image(vis_file, caption=os.path.basename(vis_file))


def train_model_ui():
    """è®­ç»ƒæ¨¡å‹ç•Œé¢"""
    st.write("## æ¨¡å‹è®­ç»ƒ")
    st.write("ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†è®­ç»ƒé¢éƒ¨æ›¿æ¢æ¨¡å‹ã€‚")
    
    # è®¾ç½®è®­ç»ƒå‚æ•°
    dataset_dir = st.text_input("æ•°æ®é›†ç›®å½•", "data/dataset")
    epochs = st.slider("è®­ç»ƒè½®æ•°", 10, 200, 50)
    
    # è®­ç»ƒæŒ‰é’®
    if st.button("å¼€å§‹è®­ç»ƒ"):
        # æ£€æŸ¥æ•°æ®é›†ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(dataset_dir):
            st.error(f"âš ï¸ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {dataset_dir}")
            st.info("è¯·å…ˆåˆ›å»ºæ•°æ®é›†ç›®å½•ï¼Œå¹¶æ”¾å…¥äººè„¸å›¾åƒ")
            return
        
        # å¼€å§‹è®­ç»ƒ
        with st.spinner("æ­£åœ¨è®­ç»ƒæ¨¡å‹..."):
            try:
                # è®­ç»ƒæ¨¡å‹
                history = st.session_state.processor.train_model(dataset_dir, epochs=epochs)
                
                # æ˜¾ç¤ºè®­ç»ƒç»“æœ
                st.success("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
                
                # åŠ è½½æ–°æ¨¡å‹
                st.session_state.model_loaded = False
                load_model()
            except Exception as e:
                st.error(f"âš ï¸ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®é¡µé¢æ ‡é¢˜
    st.set_page_config(
        page_title="è§†é¢‘äººç‰©é¢éƒ¨æ›¿æ¢ç³»ç»Ÿ",
        page_icon="ğŸ­",
        layout="wide"
    )
    
    # åº”ç”¨æ ‡é¢˜
    st.title("ğŸ­ è§†é¢‘äººç‰©é¢éƒ¨æ›¿æ¢ç³»ç»Ÿ")
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    init_session_state()
    
    # ä¾§è¾¹æ  - ç³»ç»ŸçŠ¶æ€å’Œæ§åˆ¶
    with st.sidebar:
        st.header("ç³»ç»Ÿæ§åˆ¶")
        
        # åŠ è½½æ¨¡å‹
        if st.button("åŠ è½½/é‡ç½®æ¨¡å‹"):
            load_model()
        
        # æ˜¾ç¤ºæ¨¡å‹çŠ¶æ€
        if st.session_state.model_loaded:
            st.success("âœ… æ¨¡å‹å·²åŠ è½½")
        else:
            st.warning("âš ï¸ æ¨¡å‹æœªåŠ è½½")
        
        # åˆ†éš”çº¿
        st.divider()
        
        # å…³äºä¿¡æ¯
        st.subheader("å…³äº")
        st.markdown("""
        æœ¬ç³»ç»ŸåŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯å®ç°è§†é¢‘ä¸­çš„äººç‰©é¢éƒ¨æ›¿æ¢ã€‚
        
        ä¸»è¦åŠŸèƒ½:
        - è§†é¢‘ä¸­äººè„¸æ£€æµ‹ä¸è·Ÿè¸ª
        - åŸºäºæ·±åº¦å­¦ä¹ çš„é¢éƒ¨æ›¿æ¢
        - çœ¼éƒ¨ä¼˜åŒ–ä¸è¡¨æƒ…åŒ¹é…
        - é«˜è´¨é‡åå¤„ç†ä¸èåˆ
        """)
    
    # ä¸»è¦åŠŸèƒ½åŒº
    tab1, tab2, tab3, tab4 = st.tabs(["æºä¸ç›®æ ‡", "å•å¸§é¢„è§ˆ", "è§†é¢‘å¤„ç†", "æ¨¡å‹è®­ç»ƒ"])
    
    with tab1:
        # åˆ›å»ºä¸¤åˆ—å¸ƒå±€ï¼Œå·¦è¾¹ä¸Šä¼ æºäººè„¸ï¼Œå³è¾¹ä¸Šä¼ ç›®æ ‡è§†é¢‘
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ç¬¬ä¸€æ­¥ï¼šä¸Šä¼ æºäººè„¸")
            upload_source_face()
        
        with col2:
            st.subheader("ç¬¬äºŒæ­¥ï¼šä¸Šä¼ ç›®æ ‡è§†é¢‘")
            video_uploaded = upload_target_video()
            
            if video_uploaded:
                st.success("âœ… æºäººè„¸å’Œç›®æ ‡è§†é¢‘å‡å·²ä¸Šä¼ ï¼")
                st.info("è¯·ç‚¹å‡»ä¸Šæ–¹çš„\"å•å¸§é¢„è§ˆ\"æˆ–\"è§†é¢‘å¤„ç†\"é€‰é¡¹å¡ç»§ç»­æ“ä½œã€‚")
    
    with tab2:
        process_single_frame()
    
    with tab3:
        process_video()
    
    with tab4:
        train_model_ui()


if __name__ == "__main__":
    main() 